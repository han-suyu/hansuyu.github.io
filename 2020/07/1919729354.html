<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>基于DenseNet模型实现MNIST手写体数据集的训练 | 明天又是周六了</title><meta name="description" content="原论文：《Densely Connected Convolutional Networks》                   原论文：https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1608.06993.pdf 下载好的：https:&#x2F;&#x2F;wwa.lanzous.com&#x2F;iHEsaebfwwj            DenseNet 网络是在 2017 的论文 《Dens"><meta name="keywords" content="CNN,经典卷积网络模型"><meta name="author" content="Seven"><meta name="copyright" content="Seven"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://hansy.tech/2020/07/1919729354.html"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//www.google-analytics.com" crossorigin="crossorigin"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//ta.qq.com"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin="crossorigin"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="preconnect" href="//zz.bdstatic.com"/><meta name="google-site-verification" content="uu5kdlJe9WdJFhVerEPHYM3WWZLf7P8rSd77zi6nywQ"/><meta name="baidu-site-verification" content="m7BzC4y6nU"/><meta property="og:type" content="article"><meta property="og:title" content="基于DenseNet模型实现MNIST手写体数据集的训练"><meta property="og:url" content="https://hansy.tech/2020/07/1919729354.html"><meta property="og:site_name" content="明天又是周六了"><meta property="og:description" content="原论文：《Densely Connected Convolutional Networks》                   原论文：https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1608.06993.pdf 下载好的：https:&#x2F;&#x2F;wwa.lanzous.com&#x2F;iHEsaebfwwj            DenseNet 网络是在 2017 的论文 《Dens"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/han-suyu/cover/9.jpg"><meta property="article:published_time" content="2020-07-02T09:21:03.000Z"><meta property="article:modified_time" content="2020-07-24T09:57:53.625Z"><meta name="twitter:card" content="summary"><script>var activateDarkMode = function () {
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#000')
  }
}
var activateLightMode = function () {
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#fff')
  }
}

var getCookies = function (name) {
  const value = `; ${document.cookie}`
  const parts = value.split(`; ${name}=`)
  if (parts.length === 2) return parts.pop().split(';').shift()
}

var autoChangeMode = 'false'
var t = getCookies('theme')
if (autoChangeMode === '1') {
  var isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
  var isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
  var isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
  var hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined) {
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport) {
      console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
      var now = new Date()
      var hour = now.getHours()
      var isNight = hour <= 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
    }
    window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
      if (Cookies.get('theme') === undefined) {
        e.matches ? activateDarkMode() : activateLightMode()
      }
    })
  } else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else if (autoChangeMode === '2') {
  now = new Date()
  hour = now.getHours()
  isNight = hour <= 6 || hour >= 18
  if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else {
  if (t === 'dark') activateDarkMode()
  else if (t === 'light') activateLightMode()
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css"><link rel="prev" title="YOLO v1学习总结" href="https://hansy.tech/2020/07/3774741536.html"><link rel="next" title="基于ResNet模型实现MNIST手写体数据集的训练" href="https://hansy.tech/2020/06/2398542062.html"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?8a00fdda614f4a870fbc5f2d54aa8666";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-173466034-1', 'auto');
ga('send', 'pageview');
</script><script src="https://tajs.qq.com/stats?sId=66574834" charset="UTF-8"></script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  translate: {"defaultEncoding":1,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: {"languages":{"author":"作者: Seven","link":"链接: ","source":"来源: 明天又是周六了","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: {"bookmark":{"message_prev":"按","message_next":"键将本页加入书签"},"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#2d3035","position":"top-center"},
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  baiduPush: true,
  highlightCopy: true,
  highlightLang: true,
  isPhotoFigcaption: true,
  islazyload: true,
  isanchor: false    
}</script><script>var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: true,
  isSidebar: true
  }</script><noscript><style>
#nav {
  opacity: 1
}
.justified-gallery img{
  opacity: 1
}
</style></noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sviptzk/StaticFile_HEXO@latest/butterfly/css/flink.min.css"><link rel="stylesheet" href="/css/my.css"><link rel="stylesheet" href="http://at.alicdn.com/t/font_1927020_azxlhjmymca.css"><link href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css" rel="stylesheet" type="text/css"><meta name="generator" content="Hexo 4.2.1"></head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">161</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">34</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">20</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-calendar"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tag"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/photos/"><i class="fa-fw fa fa-camera"></i><span> 相册</span></a></div><div class="menus_item"><a class="site-page" href="/movies/"><i class="fa-fw fa fa-film"></i><span> 电影</span></a></div><div class="menus_item"><a class="site-page" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></div><div class="menus_item"><a class="site-page" href="/artitalk/"><i class="fa-fw fa fa-edit"></i><span> 心情</span></a></div><div class="menus_item"><a class="site-page" href="/toolbox/"><i class="fa-fw fa fa-leaf"></i><span> 小工具</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fa fa-institution"></i><span> 实验室</span><i class="fas fa-chevron-down menus-expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/color/"><i class="fa-fw fa fa-magic"></i><span> RGB颜色</span></a></li><li><a class="site-page" href="/hexconvert/"><i class="fa-fw fa fa-calculator"></i><span> 进制转换</span></a></li><li><a class="site-page" href="/diff/"><i class="fa-fw fa fa-clone"></i><span> 文本对比</span></a></li><li><a class="site-page" href="/map/"><i class="fa-fw fa fa-globe"></i><span> 地球图层</span></a></li><li><a class="site-page" href="/dog/"><i class="fa-fw fa fa-heartbeat"></i><span> 舔狗日记</span></a></li></ul></div><div class="menus_item"><a class="site-page"><i class="fa-fw fa fa-game"></i><span> 小游戏</span><i class="fas fa-chevron-down menus-expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/Sudoku/"><span> 数独</span></a></li><li><a class="site-page" href="/jumper/"><span> 跳一跳</span></a></li><li><a class="site-page" href="/puzzleNumber/"><span> 数字拼图</span></a></li><li><a class="site-page" href="/referrence/"><span> referrence</span></a></li></ul></div></div></div></div><i class="fas fa-arrow-right" id="toggle-sidebar"></i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#一-设计理念"><span class="toc-text"> 一、设计理念</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#二-与resnet的比较"><span class="toc-text"> 二、与ResNet的比较</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#三-网络结构"><span class="toc-text"> 三、网络结构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#31-denseblock"><span class="toc-text"> 3.1 DenseBlock</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#32-transition-layer"><span class="toc-text"> 3.2 Transition layer</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#四-densenet特点"><span class="toc-text"> 四、DenseNet特点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#五-densenet性能与结果分析"><span class="toc-text"> 五、DenseNet性能与结果分析</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#51-性能"><span class="toc-text"> 5.1 性能</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#52-反向传播分析"><span class="toc-text"> 5.2 反向传播分析</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#六-代码实现"><span class="toc-text"> 六、代码实现</span></a></li></ol></div></div></div><div class="code-close" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://cdn.jsdelivr.net/gh/han-suyu/cover/9.jpg)"><nav id="nav"><span class="pull-left" id="blog_name"><a class="blog_title" id="site-name" href="/">明天又是周六了</a></span><span class="pull-right menus"><div id="search_button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-calendar"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tag"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/photos/"><i class="fa-fw fa fa-camera"></i><span> 相册</span></a></div><div class="menus_item"><a class="site-page" href="/movies/"><i class="fa-fw fa fa-film"></i><span> 电影</span></a></div><div class="menus_item"><a class="site-page" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></div><div class="menus_item"><a class="site-page" href="/artitalk/"><i class="fa-fw fa fa-edit"></i><span> 心情</span></a></div><div class="menus_item"><a class="site-page" href="/toolbox/"><i class="fa-fw fa fa-leaf"></i><span> 小工具</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fa fa-institution"></i><span> 实验室</span><i class="fas fa-chevron-down menus-expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/color/"><i class="fa-fw fa fa-magic"></i><span> RGB颜色</span></a></li><li><a class="site-page" href="/hexconvert/"><i class="fa-fw fa fa-calculator"></i><span> 进制转换</span></a></li><li><a class="site-page" href="/diff/"><i class="fa-fw fa fa-clone"></i><span> 文本对比</span></a></li><li><a class="site-page" href="/map/"><i class="fa-fw fa fa-globe"></i><span> 地球图层</span></a></li><li><a class="site-page" href="/dog/"><i class="fa-fw fa fa-heartbeat"></i><span> 舔狗日记</span></a></li></ul></div><div class="menus_item"><a class="site-page"><i class="fa-fw fa fa-game"></i><span> 小游戏</span><i class="fas fa-chevron-down menus-expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/Sudoku/"><span> 数独</span></a></li><li><a class="site-page" href="/jumper/"><span> 跳一跳</span></a></li><li><a class="site-page" href="/puzzleNumber/"><span> 数字拼图</span></a></li><li><a class="site-page" href="/referrence/"><span> referrence</span></a></li></ul></div></div><span class="toggle-menu close"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><div id="post-title"><div class="posttitle">基于DenseNet模型实现MNIST手写体数据集的训练</div></div><div id="post-meta"><div class="meta-firstline"><time class="post-meta__date"><span class="post-meta__date-created" title="发表于 2020-07-02 17:21:03"><i class="far fa-calendar-alt fa-fw"></i> 发表于 2020-07-02</span><span class="post-meta__separator">|</span><span class="post-meta__date-updated" title="更新于 2020-07-24 17:57:53"><i class="fas fa-history fa-fw"></i> 更新于 2020-07-24</span></time><span class="post-meta__categories"><span class="post-meta__separator">|</span><i class="fas fa-inbox fa-fw post-meta__icon"></i><a class="post-meta__categories" href="/categories/Computer-Version/">Computer Version</a><i class="fas fa-angle-right post-meta__separator"></i><i class="fas fa-inbox fa-fw post-meta__icon"></i><a class="post-meta__categories" href="/categories/Computer-Version/%E7%BB%8F%E5%85%B8%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/">经典卷积网络模型</a></span></div><div class="meta-secondline"> <span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta__icon"></i><span>字数总计:</span><span class="word-count">4.8k</span><span class="post-meta__separator">|</span><i class="far fa-clock fa-fw post-meta__icon"></i><span>阅读时长: 19 分钟</span></span></div><div class="meta-thirdline"><span class="post-meta-pv-cv"><span class="post-meta__separator">|</span><i class="far fa-eye fa-fw post-meta__icon"></i><span>阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-commentcount"><span class="post-meta__separator">|</span><i class="far fa-comments fa-fw post-meta__icon"></i><span>评论数:</span><a href="/2020/07/1919729354.html#post-comment" itemprop="discussionUrl"><span class="valine-comment-count comment-count" data-xid="/2020/07/1919729354.html" itemprop="commentCount"></span></a></span></div></div></div></header><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><div class='spoiler collapsed'>
    <div class='spoiler-title'>
        原论文：《Densely Connected Convolutional Networks》
    </div>
    <div class='spoiler-content'>
        <p><strong>原论文</strong>：<a href="https://arxiv.org/pdf/1608.06993.pdf" target="_blank" rel="noopener external nofollow noreferrer">https://arxiv.org/pdf/1608.06993.pdf</a><br />
<strong>下载好的</strong>：<a href="https://wwa.lanzous.com/iHEsaebfwwj" target="_blank" rel="noopener external nofollow noreferrer">https://wwa.lanzous.com/iHEsaebfwwj</a></p>

    </div>
</div>
<p> </p>
<p> </p>
<p>DenseNet 网络是在 2017 的论文 <a href="https://arxiv.org/pdf/1608.06993.pdf" target="_blank" rel="noopener external nofollow noreferrer">《Densely Connected Convolutional Networks》</a> 中提出，并斩获CVPR 2017年最佳论文奖。DenseNet 是一种具有密集连接的卷积神经网络，在该网络中，任何两层之间都有直接的连接，也就是说，网络每一层的输入包括其前面所有层，而该层所学习的特征也会直接输出给其后面所有层。</p>
<p> </p>
<p> </p>
<h2 id="一-设计理念"><a class="markdownIt-Anchor" href="#一-设计理念"></a> 一、设计理念</h2>
<p>它的基本思路与ResNet一致，但是它建立的是当前层与前面所有层的<strong>密集连接</strong>（dense connection），它的名称也是由此而来。DenseNet的另一大特色是通过特征在channel上的连接来实现<strong>特征重用</strong>（feature reuse）来代替 ResNet 的 Element-wise addition。</p>
<p> </p>
<p>密集连接真的不会带来冗余吗？（<a href="https://www.cnblogs.com/skyfsm/p/8451834.html" target="_blank" rel="noopener external nofollow noreferrer">参考</a>）</p>
<p>不会的。密集连接这个词给人的第一感觉就是极大的增加了网络的参数量和计算量。但实际上 DenseNet 比其他网络效率更高，其关键就在于网络每层计算量的减少以及特征的重复利用。DenseNet是让第一层的输入直接影响到之后的所有层，它的输出为：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mi>l</mi></msub><mo>=</mo><msub><mi>H</mi><mi>l</mi></msub><mo stretchy="false">(</mo><mo stretchy="false">[</mo><msub><mi>x</mi><mn>0</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>x</mi><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">]</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">x_l=H_l([x_0,x_1,…,x_{l-1}])</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mopen">[</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mclose">]</span><span class="mclose">)</span></span></span></span>，其中 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo stretchy="false">[</mo><msub><mi>x</mi><mn>0</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>x</mi><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[x_0,x_1,…,x_{l-1}]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mclose">]</span></span></span></span> 是将之前的feature map以通道的维度进行合并。并且由于每一层都包含之前所有层的输出信息，因此其只需要很少的特征图就够了，这也是为什么DneseNet的参数量较其他模型大大减少的原因。这种dense connection相当于每一层都直接连接input和loss，因此就可以减轻梯度消失现象，这样加深网络不是问题 。</p>
<p>需要明确一点，dense connectivity 仅仅是在一个dense block里的，不同dense block 之间是没有dense connectivity的。</p>
<p> </p>
<p> </p>
<h2 id="二-与resnet的比较"><a class="markdownIt-Anchor" href="#二-与resnet的比较"></a> 二、与ResNet的比较</h2>
<p>相比ResNet，DenseNet提出了一个更激进的密集连接机制：即互相连接所有的层，具体来说就是每个层都会接受其前面所有层作为其额外的输入。<a href="https://zhuanlan.zhihu.com/p/37189203" target="_blank" rel="noopener external nofollow noreferrer">图1</a>为ResNet网络的连接机制，作为对比，<a href="https://zhuanlan.zhihu.com/p/37189203" target="_blank" rel="noopener external nofollow noreferrer">图2</a>为DenseNet的密集连接机制。可以看到，ResNet是每个层与前面的某层（一般是2~3层）短路连接在一起，连接方式是通过元素级相加。而在DenseNet中，每个层都会与前面所有层在channel维度上连接（concat）在一起（这里各个层的特征图大小是相同的——使用Transition layer），并作为下一层的输入。对于一个<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>L</mi></mrow><annotation encoding="application/x-tex">L</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault">L</span></span></span></span> 层的网络，DenseNet共包含 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mrow><mi>L</mi><mo stretchy="false">(</mo><mi>L</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><mn>2</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{L(L+1)}{2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.355em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.01em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.485em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">L</span><span class="mopen mtight">(</span><span class="mord mathdefault mtight">L</span><span class="mbin mtight">+</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span> 个连接，相比ResNet，这是一种密集连接。而且DenseNet是直接concat来自不同层的特征图，这可以实现特征重用，极大地减少了参数，提升了效率，这一特点是DenseNet与ResNet最主要的区别。</p>
<p><img src= "/img/loading.gif" data-src="https://s1.ax1x.com/2020/07/06/UC2lrR.jpg" alt="图1：ResNet网络的短路连接机制（其中+代表的是元素级相加操作）" /></p>
<p><img src= "/img/loading.gif" data-src="https://s1.ax1x.com/2020/07/06/UC21q1.jpg" alt="图2：DenseNet网络的密集连接机制（其中c代表的是channel级连接操作）" /></p>
<p>由上图可以发现两个主要区别：</p>
<ul>
<li>DenseNet 是密集连接，当前层与所有之前层都有连接；ResNet 只有相邻层有连接</li>
<li>DenseNet 是 channel-wise concatenation； Resnet 是 Element-wise addition</li>
</ul>
<p> </p>
<p>Dense Block 类似于 ResNet 中的 residual block，其区别对比如下（出自原论文）：</p>
<blockquote>
<p>Crucially, in contrast to ResNets, we never combine features <strong>summation</strong> before they are passed into a layer; instead, we combine features by <strong>concatenating</strong> them.</p>
</blockquote>
<p> </p>
<p>单点分析的话，如<a href="https://d2l.ai/chapter_convolutional-modern/densenet.html" target="_blank" rel="noopener external nofollow noreferrer">图3</a>所示。</p>
<p><img src= "/img/loading.gif" data-src="https://d2l.ai/_images/densenet-block.svg" alt="图3：The main difference between ResNet (left) and DenseNet (right) in cross-layer connections: use of addition and use of concatenation." /></p>
<p>可以看出一个是相加<code>“+”</code> 	；	一个是连接 <code>“[,]”</code></p>
<p> </p>
<p> </p>
<p> </p>
<h2 id="三-网络结构"><a class="markdownIt-Anchor" href="#三-网络结构"></a> 三、网络结构</h2>
<p>CNN网络一般要经过Pooling或者stride&gt;1的Conv来降低特征图的大小，而DenseNet的密集连接方式需要特征图大小保持一致。为了解决这个问题，DenseNet网络中使用<strong>DenseBlock Block  +  Transition Layer</strong>的结构（如<a href="https://github.com/madobet/webooru/issues/173" target="_blank" rel="noopener external nofollow noreferrer">图4</a>），其中DenseBlock是包含很多层的模块，每个层的特征图大小相同，层与层之间采用密集连接方式。而Transition模块是连接两个相邻的DenseBlock，并且通过Pooling使特征图大小降低。</p>
<p><img src= "/img/loading.gif" data-src="https://camo.githubusercontent.com/7f40b8f702e813c2c6a59a8f0f180751fca5864c/68747470733a2f2f696d67323031382e636e626c6f67732e636f6d2f626c6f672f313438333737332f3230313931302f313438333737332d32303139313030343230333131323136312d313039353937363830392e706e67" alt="图4：DenseNet结构组成" /></p>
<p> </p>
<h3 id="31-denseblock"><a class="markdownIt-Anchor" href="#31-denseblock"></a> 3.1 DenseBlock</h3>
<p>在每个Dense Block内部，<strong>每个卷积层可以知道前面所有卷积层输出的feature map是什么，因为它的输入为前面所有卷积层输出的feature map拼接而成，换个角度说，每个卷积层得到的feature map要输出给它后面所有的卷积层。<strong>这里说“每个卷积层”并不准确，<strong>更准确的说法应该是“每组卷积”</strong>，后面将看到，一组卷积是由1个 <code>1x11x1</code>卷积层和 1个<code>3x33x3</code>卷积层堆叠而成，即</strong>bottleneck结构</strong>。见论文：</p>
<blockquote>
<p><strong>to ensure maximum information flow between layers in the network</strong>, we connect all layers (with matching feature-map sizes) directly with each other. To preserve the feed-forward nature, each layer obtains additional inputs from all preceding layers and passes on its own feature-maps to all subsequent layers.</p>
</blockquote>
<p>下面看一个Dense Block的示例，如<a href="https://arxiv.org/abs/1608.06993" target="_blank" rel="noopener external nofollow noreferrer">图5</a>所示。</p>
<p><img src= "/img/loading.gif" data-src="https://cdn.jsdelivr.net/gh/han-suyu/cdn_others/A%20dense%20block%20with%205%20layers%20and%20growth%20rate%204.jpg" alt="图5：A 5-layer dense block with a growth rate of k = 4. Each layer takes all preceding feature-maps as input." /></p>
<p>图中的 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">x</span></span></span></span> 为feature map，特别的，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">x_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 为网络初始输入，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.08125em;">H</span></span></span></span> 代表非线性转化函数，它是一个组合操作，其可能包括一系列的BN，ReLU，Pooling以及Conv操作，其结构可以描述为：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>B</mi><mi>N</mi><mo>→</mo><mi>R</mi><mi>e</mi><mi>L</mi><mi>U</mi><mo>→</mo><mi>C</mi><mi>o</mi><mi>n</mi><mi>v</mi><mo stretchy="false">(</mo><mn>3</mn><mo>×</mo><mn>3</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">BN→ReLU→Conv(3\times3)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="mord mathdefault">e</span><span class="mord mathdefault">L</span><span class="mord mathdefault" style="margin-right:0.10903em;">U</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="mord mathdefault">o</span><span class="mord mathdefault">n</span><span class="mord mathdefault" style="margin-right:0.03588em;">v</span><span class="mopen">(</span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">3</span><span class="mclose">)</span></span></span></span></span></p>
<p>如 <a href="https://zhuanlan.zhihu.com/p/37189203" target="_blank" rel="noopener external nofollow noreferrer">图6</a> 所示。另外值得注意的一点是，与ResNet不同，所有DenseBlock中各个层卷积之后均输出 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span></span></span></span> 个特征图，即得到的特征图的channel数为 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span></span></span></span> ，或者说采用 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span></span></span></span> 个卷积核。Dense Block 中每层输出的 feature maps 的 Channel 数。（对于ResNet，所有 Dense Block的growth rate均相同）。一般情况下使用较小的 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span></span></span></span>（比如12），就可以得到较佳的性能。假定输入层的特征图的channel数为 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>k</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">k_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03148em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> ，那么 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>l</mi></mrow><annotation encoding="application/x-tex">l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span></span></span></span>层输入的channel数为 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>k</mi><mn>0</mn></msub><mo>+</mo><mi>k</mi><mo stretchy="false">(</mo><mi>l</mi><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">k_0+k(l-1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03148em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">)</span></span></span></span>，因此随着层数增加，尽管 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span></span></span></span> 设定得较小，DenseBlock的输入会非常多，不过这是由于特征重用所造成的，每个层仅有 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span></span></span></span> 个特征是自己独有的。</p>
<p><img src= "/img/loading.gif" data-src="https://s1.ax1x.com/2020/07/06/UC2u24.jpg" alt="图6：DenseBlock中的非线性转换结构" /></p>
<p>由于后面层的输入会非常大，DenseBlock内部可以采用bottleneck层来减少计算量，主要是原有的结构中增加 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn><mo>∗</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">1*1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span> Conv，如<a href="https://zhuanlan.zhihu.com/p/37189203" target="_blank" rel="noopener external nofollow noreferrer">图7</a>所示，其结构可以描述为：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>B</mi><mi>N</mi><mi>B</mi><mi>N</mi><mo>→</mo><mi>R</mi><mi>e</mi><mi>L</mi><mi>U</mi><mo>→</mo><mi>C</mi><mi>o</mi><mi>n</mi><mi>v</mi><mo stretchy="false">(</mo><mn>1</mn><mo>×</mo><mn>1</mn><mo stretchy="false">)</mo><mo>→</mo><mi>B</mi><mi>N</mi><mo>→</mo><mi>R</mi><mi>e</mi><mi>L</mi><mi>U</mi><mo>→</mo><mi>C</mi><mi>o</mi><mi>n</mi><mi>v</mi><mo stretchy="false">(</mo><mn>3</mn><mo>×</mo><mn>3</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">BNBN→ReLU→Conv(1\times1)→BN→ReLU→Conv(3\times3)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="mord mathdefault">e</span><span class="mord mathdefault">L</span><span class="mord mathdefault" style="margin-right:0.10903em;">U</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="mord mathdefault">o</span><span class="mord mathdefault">n</span><span class="mord mathdefault" style="margin-right:0.03588em;">v</span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="mord mathdefault">e</span><span class="mord mathdefault">L</span><span class="mord mathdefault" style="margin-right:0.10903em;">U</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="mord mathdefault">o</span><span class="mord mathdefault">n</span><span class="mord mathdefault" style="margin-right:0.03588em;">v</span><span class="mopen">(</span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">3</span><span class="mclose">)</span></span></span></span></span></p>
<p>称为DenseNet-B结构。其中1x1 Conv得到  <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>4</mn><mo>∗</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">4*k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">4</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span></span></span></span> 个特征图它起到的作用是降低特征数量，从而提升计算效率。</p>
<p><img src= "/img/loading.gif" data-src="https://s1.ax1x.com/2020/07/06/UC2nGF.jpg" alt="图7：使用bottleneck层的DenseBlock结构" /></p>
<p>在具体实现上，</p>
<p><font color=blue>在ResNet中</font>，第 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>l</mi></mrow><annotation encoding="application/x-tex">l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span></span></span></span> 层的输入 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">x_{l-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.638891em;vertical-align:-0.208331em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span></span></span></span> 经过层的转换函数 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>H</mi><mi>l</mi></msub></mrow><annotation encoding="application/x-tex">H_l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 后得到对应的输出 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>H</mi><mi>l</mi></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">H_l(x_{l-1})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> ，该输出与输入 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">x_{l-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.638891em;vertical-align:-0.208331em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span></span></span></span> 的线性组合就成了下一层的输入 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mi>l</mi></msub></mrow><annotation encoding="application/x-tex">x_l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> ，即：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mi>l</mi></msub><mo>=</mo><msub><mi>H</mi><mi>l</mi></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>l</mi><mi mathvariant="normal">−</mi><mn>1</mn></mrow></msub><mo stretchy="false">)</mo><mo>+</mo><msub><mi>x</mi><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">x_l=H_l(x_{l−1})+x_{l-1}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mord mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.638891em;vertical-align:-0.208331em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span></span></span></span></span></p>
<p><font color=blue>而在DenseNet中</font>，第 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>l</mi></mrow><annotation encoding="application/x-tex">l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span></span></span></span> 层的新增输入 与 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">x_{l-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.638891em;vertical-align:-0.208331em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span></span></span></span> 之前的所有输入 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mn>0</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>x</mi><mrow><mi>l</mi><mi mathvariant="normal">−</mi><mn>3</mn></mrow></msub><mo separator="true">,</mo><msub><mi>x</mi><mrow><mi>l</mi><mi mathvariant="normal">−</mi><mn>2</mn></mrow></msub></mrow><annotation encoding="application/x-tex">x_0,x_1,…,x_{l−3},x_{l−2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.638891em;vertical-align:-0.208331em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mord mtight">−</span><span class="mord mtight">3</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mord mtight">−</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span></span></span></span> 按照通道拼接在一起组成真正的输入，即 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo stretchy="false">[</mo><msub><mi>x</mi><mn>0</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>x</mi><mrow><mi>l</mi><mi mathvariant="normal">−</mi><mn>2</mn></mrow></msub><mo separator="true">,</mo><msub><mi>x</mi><mrow><mi>l</mi><mi mathvariant="normal">−</mi><mn>1</mn></mrow></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[x_0,x_1,…,x_{l−2},x_{l−1}]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mord mtight">−</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mord mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mclose">]</span></span></span></span> ，该输入经过一个Batch Normalization层、ReLU和卷积层得到对应的隐层输出 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>H</mi><mi>l</mi></msub></mrow><annotation encoding="application/x-tex">H_l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> ，该隐层输出就是下一层的新增输入 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mi>l</mi></msub></mrow><annotation encoding="application/x-tex">x_l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> ，即：</p>
<p>​</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mi>l</mi></msub><mo>=</mo><msub><mi>H</mi><mi>l</mi></msub><mo stretchy="false">(</mo><mo stretchy="false">[</mo><msub><mi>x</mi><mn>0</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>x</mi><mrow><mi>l</mi><mi mathvariant="normal">−</mi><mn>2</mn></mrow></msub><mo separator="true">,</mo><msub><mi>x</mi><mrow><mi>l</mi><mi mathvariant="normal">−</mi><mn>1</mn></mrow></msub><mo stretchy="false">]</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">x_l=H_l([x_0,x_1,…,x_{l−2},x_{l−1}])
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mopen">[</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mord mtight">−</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mord mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mclose">]</span><span class="mclose">)</span></span></span></span></span></p>
<p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mi>l</mi></msub></mrow><annotation encoding="application/x-tex">x_l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 再与之前的所有输入拼接为作 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo stretchy="false">[</mo><msub><mi>x</mi><mn>0</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>x</mi><mrow><mi>l</mi><mi mathvariant="normal">−</mi><mn>2</mn></mrow></msub><mo separator="true">,</mo><msub><mi>x</mi><mrow><mi>l</mi><mi mathvariant="normal">−</mi><mn>1</mn></mrow></msub><mo separator="true">,</mo><msub><mi>x</mi><mi>l</mi></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[x_0,x_1,…,x_{l−2},x_{l−1},x_l]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mord mtight">−</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mord mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">]</span></span></span></span> 为下一层的输入。</p>
<p>（ 注意这里 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>l</mi></mrow><annotation encoding="application/x-tex">l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span></span></span></span> 层与 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">l-1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.77777em;vertical-align:-0.08333em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span> 层之间可能实际上包含多个卷积层。 ）</p>
<p> </p>
<p> </p>
<h3 id="32-transition-layer"><a class="markdownIt-Anchor" href="#32-transition-layer"></a> 3.2 Transition layer</h3>
<p>DenseNet 的 Transition layer 主要是用来降低 feature map 的尺寸，将来自不同层的 feature map 变化为同等尺寸后进行 concatenate，其结构如下：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>B</mi><mi>N</mi><mo>→</mo><mi>R</mi><mi>e</mi><mi>L</mi><mi>U</mi><mo>→</mo><mi>C</mi><mi>o</mi><mi>n</mi><mi>v</mi><mo stretchy="false">(</mo><mn>1</mn><mo>×</mo><mn>1</mn><mo stretchy="false">)</mo><mo>→</mo><mi>A</mi><mi>v</mi><mi>e</mi><mi>P</mi><mi>o</mi><mi>o</mi><mi>l</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo stretchy="false">(</mo><mn>2</mn><mo>×</mo><mn>2</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">BN→ReLU→Conv(1\times1) → AvePooling(2\times2)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="mord mathdefault">e</span><span class="mord mathdefault">L</span><span class="mord mathdefault" style="margin-right:0.10903em;">U</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="mord mathdefault">o</span><span class="mord mathdefault">n</span><span class="mord mathdefault" style="margin-right:0.03588em;">v</span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">A</span><span class="mord mathdefault" style="margin-right:0.03588em;">v</span><span class="mord mathdefault">e</span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mord mathdefault">o</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">i</span><span class="mord mathdefault">n</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">2</span><span class="mclose">)</span></span></span></span></span></p>
<p>其中 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn><mo>∗</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">1*1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span> 的卷积主要用于瘦身，即降低通道数量。</p>
<p>可见，<strong>bottleneck和Transition Layer的作用都是为了提高计算效率以及压缩参数量。</strong></p>
<p> </p>
<p>如<a href="https://arxiv.org/pdf/1608.06993.pdf" target="_blank" rel="noopener external nofollow noreferrer">图8</a>所示，是包含三个Dense Block的DenseNet模型。</p>
<p><img src= "/img/loading.gif" data-src="https://cdn.jsdelivr.net/gh/han-suyu/cdn_others/A%20deep%20DenseNet%20with%20three%20dense%20blocks.jpg" alt="图8：A deep DenseNet with three denseblocks. The layers between two adjacent blocks are referred to as transition layers and change feature-map sizes via convolution and pooling." /></p>
<p> </p>
<p> </p>
<p> </p>
<h2 id="四-densenet特点"><a class="markdownIt-Anchor" href="#四-densenet特点"></a> 四、DenseNet特点</h2>
<p>综合来看，DenseNet的优势主要体现在以下几个方面：</p>
<ul>
<li>由于密集连接方式，DenseNet提升了梯度的反向传播，使得网络更容易训练。由于每层可以直达最后的误差信号，实现了隐式的“<a href="https://arxiv.org/abs/1409.5185" target="_blank" rel="noopener external nofollow noreferrer">deep supervision</a>”；</li>
<li>参数更小且计算更高效，这有点违反直觉，由于DenseNet是通过concat特征来实现短路连接，实现了特征重用，并且采用较小的growth rate，每个层所独有的特征图是比较小的；</li>
<li>由于特征复用，最后的分类器使用了低级特征。</li>
</ul>
<p> </p>
<p>要注意的一点是，如果实现方式不当的话，DenseNet可能耗费很多GPU显存</p>
<p> </p>
<p>阅读了文章<a href="https://zhuanlan.zhihu.com/p/37189203" target="_blank" rel="noopener external nofollow noreferrer">【DenseNet：比ResNet更优的CNN模型 - 小小将的文章 - 知乎】</a>下的所有评论，他们主要讨论的就是DenseNet不流行的原因，即<font color=red>对显存要求太高，相同占用下，性能远不如其他网络。</font></p>
<p> </p>
<p>正是concat操作造成了densenet出现了比resnet更密集的连接，当每层搜集特征时，就会因为这些连接而造成极大的显存占用。比如100多层的网络最后一层把dense block里的所有feature map都concat起来，单单这一步就秒杀普通显存。</p>
<p> </p>
<p>评论列表中有一位大佬<a href="https://www.zhihu.com/people/lai-shen-qi" target="_blank" rel="noopener external nofollow noreferrer"><code>小赖sqLai</code></a>的发言感觉很厉害，他的意思是：</p>
<blockquote>
<p>参数数量、计算速度、显存占用三者没有直接关系。比如squeezenet和squeezenext参数只有alexnet几百分之一，性能却持平，显存占用要大好几倍。</p>
<p>他认为显存占用与推断中所产生的feature map数目有关。有些框架会有优化，自动把比较靠前的层的feature map释放掉，所以显存就会减少，或者inplace操作通过重新计算的方法减少一部分显存，但是densenet因为需要重复利用比较靠前的feature map，所以无法释放，导致显存占用过大。</p>
</blockquote>
<p> </p>
<p>一种高效的实现如下<a href="https://zhuanlan.zhihu.com/p/37189203" target="_blank" rel="noopener external nofollow noreferrer">图9</a>所示，更多细节可以见这篇论文<a href="https://arxiv.org/abs/1707.06990" target="_blank" rel="noopener external nofollow noreferrer">《Memory-Efficient Implementation of DenseNets》</a>。</p>
<p><img src= "/img/loading.gif" data-src="https://pic4.zhimg.com/80/v2-5fd6e90863933b31140dd06bfc05b083_720w.jpg" alt="图9：DenseNet的更高效实现方式" /></p>
<p> </p>
<p> </p>
<p> </p>
<h2 id="五-densenet性能与结果分析"><a class="markdownIt-Anchor" href="#五-densenet性能与结果分析"></a> 五、<a href="https://www.cnblogs.com/shine-lee/p/12380510.html" target="_blank" rel="noopener external nofollow noreferrer">DenseNet性能与结果分析</a></h2>
<h4 id="51-性能"><a class="markdownIt-Anchor" href="#51-性能"></a> 5.1 性能</h4>
<p>DenseNet用于ImageNet的网络架构如<a href="https://arxiv.org/pdf/1608.06993.pdf" target="_blank" rel="noopener external nofollow noreferrer">表1</a>所示。</p>
<p><img src= "/img/loading.gif" data-src="https://s1.ax1x.com/2020/07/06/UC2mPU.jpg" alt="表1：DenseNet architectures for ImageNet. The growth rate for all the networks is k = 32. Note that each “conv” layer shown in the table corresponds the sequence BN-ReLU-Conv." /></p>
<p>DenseNet的<strong>Parameter Efficiency</strong>很高，<strong>可以用少得多的参数和计算复杂度，取得与ResNet相当的性能</strong>，如下<a href="https://arxiv.org/pdf/1608.06993.pdf" target="_blank" rel="noopener external nofollow noreferrer">图10</a>所示。</p>
<p><img src= "/img/loading.gif" data-src="https://s1.ax1x.com/2020/07/06/UC2QM9.jpg" alt="图10：Comparison of the DenseNets and ResNets top-1 error rates(single-crop testing) on the ImageNet validation dataset as a function of learned parameters(left) and FLOPs during test-time (right)." /></p>
<p> </p>
<p> </p>
<h4 id="52-反向传播分析"><a class="markdownIt-Anchor" href="#52-反向传播分析"></a> 5.2 反向传播分析</h4>
<p>DenseNet最终的输出为前面各层输出的拼接，在反向传播时，这种连接方式可以将最终损失直接回传到前面的各个隐藏层，相当于某种<strong>Implicit Deep Supervision</strong>，<strong>强迫各个隐藏层学习到更有区分里的特征</strong>。</p>
<p>DenseNet对feature map的使用方式可以看成是某种<strong>多尺度特征融合</strong>，文中称之为<strong>feature reuse</strong>，也可以看成是某种“<strong>延迟决定</strong>”，<strong>综合前面各环节得到的信息再决定当前层的行为</strong>。文中可视化了同block内每层对前面层的依赖程度，见论文：</p>
<blockquote>
<p>For each convolutional layer ‘ within a block, we compute the average (absolute) weight assigned to connections with layers. Figure 5 shows a heat-map for all three dense blocks. The average absolute<br />
weight serves as a surrogate for the dependency of a convolutional layer on its preceding layers.</p>
</blockquote>
<p><img src= "/img/loading.gif" data-src="https://s1.ax1x.com/2020/07/06/UC2KxJ.jpg" alt="" /></p>
<p><a href="https://arxiv.org/pdf/1608.06993.pdf" target="_blank" rel="noopener external nofollow noreferrer">图11</a>中可见每个Dense Block中每层对前面层的依赖程度，约接近红色表示依赖程度越高，可以看到，</p>
<ul>
<li>Dense Block内，每个层对其前面的feature map利用方式（依赖程度）是不一样的，相当于某种“<strong>注意力</strong>”</li>
<li>Transition Layer 以及最后的Classification Layer对其前面<strong>相对宏观的特征依赖较高</strong>，这种趋势越深越明显</li>
</ul>
<p> </p>
<p> </p>
<p> </p>
<h2 id="六-代码实现"><a class="markdownIt-Anchor" href="#六-代码实现"></a> 六、代码实现</h2>
<p>在<a href="https://hansy.tech/2020/06/2013076781.html">基于简单的全连接网络实现 MNIST 手写体数据集的训练与识别</a>的基础上重写<code>Mnist_Base</code>类，其余代码相同。</p>
<p>由于网络结构复杂，所以将搭建模型的步骤分离。程序分为<code>train.py</code>和<code>model.py</code>。运行<code>train.py</code>即可。</p>
<p><font color=blue>model.py</font></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers</span><br><span class="line"></span><br><span class="line"><span class="comment">#  二：搭建网络结构</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 搭建瓶颈层，相当于每一个稠密块中若干个相同的H函数</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BottleNeck</span><span class="params">(layers.Layer)</span>:</span></span><br><span class="line">    <span class="comment"># growth_rate对应的是论文中的增长率k，指经过一个BottleNet输出的特征图的通道数；drop_rate指失活率。</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, growth_rate, drop_rate)</span>:</span></span><br><span class="line">        super(BottleNeck, self).__init__()</span><br><span class="line">        self.bn1 = layers.BatchNormalization()</span><br><span class="line">        self.conv1 = layers.Conv2D(filters=<span class="number">4</span> * growth_rate,  <span class="comment"># 使用1*1卷积核将通道数降维到4*k</span></span><br><span class="line">                                            kernel_size=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">                                            strides=<span class="number">1</span>,</span><br><span class="line">                                            padding=<span class="string">"same"</span>)</span><br><span class="line">        self.bn2 = layers.BatchNormalization()</span><br><span class="line">        self.conv2 = layers.Conv2D(filters=growth_rate,  <span class="comment"># 使用3*3卷积核，使得输出维度（通道数）为k</span></span><br><span class="line">                                            kernel_size=(<span class="number">3</span>, <span class="number">3</span>),</span><br><span class="line">                                            strides=<span class="number">1</span>,</span><br><span class="line">                                            padding=<span class="string">"same"</span>)</span><br><span class="line">        self.dropout = layers.Dropout(rate=drop_rate)</span><br><span class="line">        <span class="comment"># 将网络层存入一个列表中</span></span><br><span class="line">        self.listLayers = [self.bn1,</span><br><span class="line">                           layers.Activation(<span class="string">"relu"</span>),</span><br><span class="line">                           self.conv1,</span><br><span class="line">                           self.bn2,</span><br><span class="line">                           layers.Activation(<span class="string">"relu"</span>),</span><br><span class="line">                           self.conv2,</span><br><span class="line">                           self.dropout]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        y = x</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.listLayers.layers:</span><br><span class="line">            y = layer(y)</span><br><span class="line">        <span class="comment"># 每经过一个BottleNet，将输入和输出按通道连结。作用是：将前l层的输入连结起来，作为下一个BottleNet的输入。</span></span><br><span class="line">        y = layers.concatenate([x, y], axis=<span class="number">-1</span>)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"><span class="comment"># 搭建稠密块，由若干个相同的瓶颈层构成</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DenseBlock</span><span class="params">(layers.Layer)</span>:</span></span><br><span class="line">    <span class="comment"># num_layers表示该稠密块存在BottleNet的个数，也就是一个稠密块的层数L</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_layers, growth_rate, drop_rate=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">        super(DenseBlock, self).__init__()</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.growth_rate = growth_rate</span><br><span class="line">        self.drop_rate = drop_rate</span><br><span class="line">        self.listLayers = []</span><br><span class="line">        <span class="comment"># 一个DenseBlock由多个相同的BottleNeck构成，我们将它们放入一个列表中。</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_layers):</span><br><span class="line">            self.listLayers.append(BottleNeck(growth_rate=self.growth_rate, drop_rate=self.drop_rate))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.listLayers.layers:</span><br><span class="line">            x = layer(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 搭建过渡层</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransitionLayer</span><span class="params">(layers.Layer)</span>:</span></span><br><span class="line">    <span class="comment"># out_channels代表输出通道数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, out_channels)</span>:</span></span><br><span class="line">        super(TransitionLayer, self).__init__()</span><br><span class="line">        self.bn = layers.BatchNormalization()</span><br><span class="line">        self.conv = layers.Conv2D(filters=out_channels,</span><br><span class="line">                                           kernel_size=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">                                           strides=<span class="number">1</span>,</span><br><span class="line">                                           padding=<span class="string">"same"</span>)</span><br><span class="line">        self.pool = layers.MaxPool2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>),   <span class="comment"># 2倍下采样</span></span><br><span class="line">                                              strides=<span class="number">2</span>,</span><br><span class="line">                                              padding=<span class="string">"same"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">        x = self.bn(inputs)</span><br><span class="line">        x = tf.keras.activations.relu(x)</span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        x = self.pool(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 搭建Mnist_ResNet整体网络结构</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Mnist_ResNet</span><span class="params">(tf.keras.Model)</span>:</span></span><br><span class="line">    <span class="comment"># num_init_features:代表初始的通道数，即输入第一个稠密块时的通道数</span></span><br><span class="line">    <span class="comment"># growth_rate:对应的是论文中的增长率k，指经过一个BottleNet输出的特征图的通道数</span></span><br><span class="line">    <span class="comment"># block_layers:每个稠密块中的BottleNet的个数</span></span><br><span class="line">    <span class="comment"># compression_rate:压缩因子，其值在(0,1]范围内</span></span><br><span class="line">    <span class="comment"># drop_rate：失活率</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_init_features, growth_rate, block_layers, compression_rate, drop_rate)</span>:</span></span><br><span class="line">        super(Mnist_ResNet, self).__init__()</span><br><span class="line">        <span class="comment"># 第一层，7*7的卷积层，2倍下采样。</span></span><br><span class="line">        self.conv = layers.Conv2D(filters=num_init_features,</span><br><span class="line">                                           kernel_size=(<span class="number">7</span>, <span class="number">7</span>),</span><br><span class="line">                                           strides=<span class="number">2</span>,</span><br><span class="line">                                           padding=<span class="string">"same"</span>)</span><br><span class="line">        self.bn = layers.BatchNormalization()</span><br><span class="line">        <span class="comment"># 最大池化层，3*3卷积核，2倍下采样</span></span><br><span class="line">        self.pool = layers.MaxPool2D(pool_size=(<span class="number">3</span>, <span class="number">3</span>), strides=<span class="number">2</span>, padding=<span class="string">"same"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 稠密块 Dense Block(1)</span></span><br><span class="line">        self.num_channels = num_init_features</span><br><span class="line">        self.dense_block_1 = DenseBlock(num_layers=block_layers[<span class="number">0</span>], growth_rate=growth_rate, drop_rate=drop_rate)</span><br><span class="line">        <span class="comment"># 该稠密块总的输出的通道数</span></span><br><span class="line">        self.num_channels += growth_rate * block_layers[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># 对特征图的通道数进行压缩</span></span><br><span class="line">        self.num_channels = compression_rate * self.num_channels</span><br><span class="line">        <span class="comment"># 过渡层1，过渡层进行下采样</span></span><br><span class="line">        self.transition_1 = TransitionLayer(out_channels=int(self.num_channels))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 稠密块 Dense Block(2)</span></span><br><span class="line">        self.dense_block_2 = DenseBlock(num_layers=block_layers[<span class="number">1</span>], growth_rate=growth_rate, drop_rate=drop_rate)</span><br><span class="line">        self.num_channels += growth_rate * block_layers[<span class="number">1</span>]</span><br><span class="line">        self.num_channels = compression_rate * self.num_channels</span><br><span class="line">        <span class="comment"># 过渡层2，2倍下采样，输出：14*14</span></span><br><span class="line">        self.transition_2 = TransitionLayer(out_channels=int(self.num_channels))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 稠密块 Dense Block(3)</span></span><br><span class="line">        self.dense_block_3 = DenseBlock(num_layers=block_layers[<span class="number">2</span>], growth_rate=growth_rate, drop_rate=drop_rate)</span><br><span class="line">        self.num_channels += growth_rate * block_layers[<span class="number">2</span>]</span><br><span class="line">        self.num_channels = compression_rate * self.num_channels</span><br><span class="line">        <span class="comment"># 过渡层3，2倍下采样</span></span><br><span class="line">        self.transition_3 = TransitionLayer(out_channels=int(self.num_channels))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 稠密块 Dense Block(4)</span></span><br><span class="line">        self.dense_block_4 = DenseBlock(num_layers=block_layers[<span class="number">3</span>], growth_rate=growth_rate, drop_rate=drop_rate)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 全局平均池化，输出size：1*1</span></span><br><span class="line">        self.avgpool = layers.GlobalAveragePooling2D()</span><br><span class="line">        <span class="comment"># 全连接层，进行10分类</span></span><br><span class="line">        self.fc = layers.Dense(units=<span class="number">10</span>, activation=tf.keras.activations.softmax)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">        x = self.conv(inputs)</span><br><span class="line">        x = self.bn(x)</span><br><span class="line">        x = tf.keras.activations.relu(x)</span><br><span class="line">        x = self.pool(x)</span><br><span class="line"></span><br><span class="line">        x = self.dense_block_1(x)</span><br><span class="line">        x = self.transition_1(x)</span><br><span class="line">        x = self.dense_block_2(x)</span><br><span class="line">        x = self.transition_2(x)</span><br><span class="line">        x = self.dense_block_3(x)</span><br><span class="line">        x = self.transition_3(x,)</span><br><span class="line">        x = self.dense_block_4(x)</span><br><span class="line"></span><br><span class="line">        x = self.avgpool(x)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">densenet</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> Mnist_ResNet(num_init_features=<span class="number">64</span>, growth_rate=<span class="number">32</span>, block_layers=[<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>], compression_rate=<span class="number">0.5</span>, drop_rate=<span class="number">0.5</span>)       <span class="comment">#Mnist数据集</span></span><br><span class="line">    <span class="comment"># return Mnist_ResNet(num_init_features=64, growth_rate=32, block_layers=[4, 4, 4, 4], compression_rate=0.5, drop_rate=0.5)  #Cifar10数据集</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">myModel=densenet()    <span class="comment">#实例化</span></span><br></pre></td></tr></table></figure>
<p><font color=blue>train.py</font></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Flatten,Dense</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">np.set_printoptions(threshold=np.inf)  <span class="comment">#设置print输出格式，通过np.inf使完全输出，不允许用省略号代替</span></span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> myModel</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#  一：导入数据集，设定训练集和测试集的特征和标签</span></span><br><span class="line">mnist = tf.keras.datasets.mnist                         <span class="comment">#下载手写数字数据集</span></span><br><span class="line">(x_train,y_train),(x_test,y_test) = mnist.load_data()   <span class="comment">#指定训练集和测试集的输入特征和标签</span></span><br><span class="line">x_train , x_test = x_train/<span class="number">255.0</span> , x_test/<span class="number">255.0</span>         <span class="comment">#对输入网络的特征进行归一化。全部转化为0到1之间的数，数值变小有利于神经网络的吸收</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#注意：如果有卷积层，数据集又是单通道灰度图片（如本数据集mnist），需要添加如下两行代码。 具体参考：https://blog.csdn.net/tushuguan_sun/article/details/105914661</span></span><br><span class="line">x_train = x_train.reshape(x_train.shape[<span class="number">0</span>], <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)  </span><br><span class="line">x_test = x_test.reshape(x_test.shape[<span class="number">0</span>], <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>) </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#  二：搭建网络结构myModel （见另一文件）</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#  三：配置训练方法</span></span><br><span class="line">myModel.compile(</span><br><span class="line">    optimizer=<span class="string">'adam'</span>,    <span class="comment">#优化器选择adam</span></span><br><span class="line">    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="literal">False</span>),  <span class="comment">#损失函数选择SparseCategoricalCrossentropy，因为前面已经保证输出满足概率分布，所以这里from_logits=False</span></span><br><span class="line">    metrics=[<span class="string">'sparse_categorical_accuracy'</span>]   <span class="comment">#数据集中标签是数值，输出结果y是概率分布，所以衡量方法选择sparse_categorical_accuracy</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#  （断点续训，存取模型参数。  在下次训练时，从之前获取的最优的参数开始，提高了准确率）</span></span><br><span class="line">    <span class="comment">#读取模型</span></span><br><span class="line">checkpoint_save_path=<span class="string">'./checkpoint/mnist.ckpt'</span></span><br><span class="line"><span class="keyword">if</span> os.path.exists(checkpoint_save_path+<span class="string">'.index'</span>):   <span class="comment">#生成ckpt文件时会自动生成索引文件，所以拿它的索引文件来判断</span></span><br><span class="line">    print(<span class="string">'------加载已有模型------'</span>)</span><br><span class="line">    myModel.load_weights(checkpoint_save_path)       <span class="comment">#如果存在模型，则直接读取</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">#保存模型</span></span><br><span class="line">cp_callback = tf.keras.callbacks.ModelCheckpoint(   <span class="comment">#使用tf给出的回调函数来保存模型参数</span></span><br><span class="line">    filepath=checkpoint_save_path,</span><br><span class="line">    save_weights_only=<span class="literal">True</span>,    <span class="comment">#是否只保留模型参数</span></span><br><span class="line">    save_best_only=<span class="literal">True</span>        <span class="comment">#是否只保留最优结果</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#  四：执行训练过程</span></span><br><span class="line">history = myModel.fit(</span><br><span class="line">    x_train,y_train,</span><br><span class="line">    batch_size=<span class="number">64</span>,                          <span class="comment">#每次喂入网络64组数据</span></span><br><span class="line">    epochs=<span class="number">10</span>,                              <span class="comment">#数据集迭代10次</span></span><br><span class="line">    validation_data=(x_test,y_test),</span><br><span class="line">    validation_freq=<span class="number">1</span>,                      <span class="comment">#每迭代一次训练集执行一次测试集的评测</span></span><br><span class="line">    callbacks=[cp_callback])                <span class="comment">#加入回调选项，返回给history。（如果不用断点续训，则不用写 “history=” 和 “callbacks=[cp_callback]” ）</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">file = open(<span class="string">'./weights.txt'</span>,<span class="string">'w'</span>)</span><br><span class="line"><span class="keyword">for</span> v <span class="keyword">in</span> myModel.trainable_variables:</span><br><span class="line">    file.write(str(v.name) + <span class="string">'\n'</span>)</span><br><span class="line">    file.write(str(v.shape) + <span class="string">'\n'</span>)</span><br><span class="line">    file.write(str(v.numpy()) + <span class="string">'\n'</span>)</span><br><span class="line">file.close()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#通过写入到txt文本的方式查看断点续训时保存的参数</span></span><br><span class="line">file = open(<span class="string">'./weights.txt'</span>,<span class="string">'w'</span>)</span><br><span class="line"><span class="keyword">for</span> v <span class="keyword">in</span> myModel.trainable_variables:</span><br><span class="line">    file.write(str(v.name) + <span class="string">'\n'</span>)</span><br><span class="line">    file.write(str(v.shape) + <span class="string">'\n'</span>)</span><br><span class="line">    file.write(str(v.numpy()) + <span class="string">'\n'</span>)</span><br><span class="line">file.close()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#  五：打印网络结构和参数信息</span></span><br><span class="line">myModel.summary()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#  六：展示acc和loss曲线 (断点续训中history里已经保存好了)</span></span><br><span class="line">acc = history.history[<span class="string">'sparse_categorical_accuracy'</span>]</span><br><span class="line">val_acc = history.history[<span class="string">'val_sparse_categorical_accuracy'</span>]</span><br><span class="line">loss = history.history[<span class="string">'loss'</span>]</span><br><span class="line">val_loss = history.history[<span class="string">'val_loss'</span>]</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">plt.plot(acc,label=<span class="string">'Training Accuracy'</span>)</span><br><span class="line">plt.plot(val_acc,label=<span class="string">'Validation Accuracy'</span>)</span><br><span class="line">plt.title(<span class="string">'Training and Validation Accuracy'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">plt.plot(loss,label=<span class="string">'Training Loss'</span>)</span><br><span class="line">plt.plot(val_loss,label=<span class="string">'Validation Loss'</span>)</span><br><span class="line">plt.title(<span class="string">'Training and Validation Loss'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p> </p>
<p> </p>
<p> </p>
<blockquote>
<p><strong>参考：</strong><br />
He K , Zhang X , Ren S , et al. Deep Residual Learning for Image Recognition[J]. 2015.<br />
<a href="https://zhuanlan.zhihu.com/p/37189203" target="_blank" rel="noopener external nofollow noreferrer">https://zhuanlan.zhihu.com/p/37189203</a><br />
<a href="https://zhuanlan.zhihu.com/p/29559120" target="_blank" rel="noopener external nofollow noreferrer">https://zhuanlan.zhihu.com/p/29559120</a><br />
<a href="https://d2l.ai/chapter_convolutional-modern/densenet.html" target="_blank" rel="noopener external nofollow noreferrer">https://d2l.ai/chapter_convolutional-modern/densenet.html</a><br />
<a href="https://www.cnblogs.com/nowgood/p/DenseNet.html" target="_blank" rel="noopener external nofollow noreferrer">https://www.cnblogs.com/nowgood/p/DenseNet.html</a><br />
<a href="https://github.com/madobet/webooru/issues/173" target="_blank" rel="noopener external nofollow noreferrer">https://github.com/madobet/webooru/issues/173</a><br />
<a href="https://www.cnblogs.com/skyfsm/p/8451834.html" target="_blank" rel="noopener external nofollow noreferrer">https://www.cnblogs.com/skyfsm/p/8451834.html</a><br />
<a href="https://www.cnblogs.com/shine-lee/p/12380510.html" target="_blank" rel="noopener external nofollow noreferrer">https://www.cnblogs.com/shine-lee/p/12380510.html</a><br />
<a href="https://blog.csdn.net/wjinjie/article/details/105900283" target="_blank" rel="noopener external nofollow noreferrer">https://blog.csdn.net/wjinjie/article/details/105900283</a><br />
<a href="https://chmx0929.gitbook.io/machine-learning/ji-suan-ji-shi-jue/ji-suan-ji-shi-jue/tu-xiang-fen-lei/densenet" target="_blank" rel="noopener external nofollow noreferrer">https://chmx0929.gitbook.io/machine-learning/ji-suan-ji-shi-jue/ji-suan-ji-shi-jue/tu-xiang-fen-lei/densenet</a></p>
</blockquote>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script></div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined" rel="external nofollow noreferrer">Seven</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://hansy.tech/2020/07/1919729354.html">https://hansy.tech/2020/07/1919729354.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noreferrer" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://hansy.tech" target="_blank">明天又是周六了</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/CNN/">CNN</a><a class="post-meta__tags" href="/tags/%E7%BB%8F%E5%85%B8%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/">经典卷积网络模型</a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/han-suyu/cover/17.jpg" data-sites="wechat,weibo,qq,facebook,twitter"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"/><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><button class="reward-button"><i class="fas fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="post-qr-code__img" src="/img/wechat.jpg" alt="微信" onclick="window.open('/img/wechat.jpg')"/><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="post-qr-code__img" src="/img/wechat.jpg" alt="支付宝" onclick="window.open('/img/wechat.jpg')"/><div class="post-qr-code__desc">支付宝</div></li></ul></div></button></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/07/3774741536.html"><img class="prev-cover" data-src="https://cdn.jsdelivr.net/gh/han-suyu/cover/15.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">YOLO v1学习总结</div></div></a></div><div class="next-post pull-right"><a href="/2020/06/2398542062.html"><img class="next-cover" data-src="https://cdn.jsdelivr.net/gh/han-suyu/cover/8.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">基于ResNet模型实现MNIST手写体数据集的训练</div></div></a></div></nav><hr><div id="post-comment"><div class="comment_headling"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div><div class="vcomment" id="vcomment"></div><script src="https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js"></script><script>var requestSetting = function (from,set) {
  var from = from
  var setting = set.split(',').filter(function(item){
  return from.indexOf(item) > -1
  });
  setting = setting.length == 0 ? from :setting;
  return setting
}

var guestInfo = requestSetting(['nick','mail','link'],'nick,mail')
var requiredFields = requestSetting(['nick','mail'],'nick,mail')

window.valine = new Valine({
  el:'#vcomment',
  appId: 'a9HNNTF9yMcoVFJrKlLyoaAY-gzGzoHsz',
  appKey: 'utzaiW0SY0JMcLNyWSQUhU0o',
  placeholder: '使用QQ号作为昵称会自动加载你的邮箱与头像哦~',
  avatar: 'wavatar',
  meta: guestInfo,
  pageSize: '10',
  lang: 'zh-CN',
  recordIP: true,
  serverURLs: '',
  emojiCDN: '',
  emojiMaps: "",
  enableQQ: true,
  requiredFields: requiredFields
});</script></div></article></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2020 By Seven</div><div class="framework-info"><span>驱动 </span><a href="https://hexo.io" target="_blank" rel="noopener external nofollow noreferrer"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener external nofollow noreferrer"><span>Butterfly</span></a></div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><button id="readmode" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font_plus" title="放大字体"><i class="fas fa-plus"></i></button><button id="font_minus" title="缩小字体"><i class="fas fa-minus"></i></button><button class="translate_chn_to_cht" id="translateLink" title="简繁转换">简</button><button id="darkmode" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" title="设置"><i class="fas fa-cog"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="scroll_to_comment fas fa-comments"></i></a><button class="close" id="mobile-toc-button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></section><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a href="https://github.com/wzpan/hexo-generator-search" target="_blank" rel="noopener external nofollow noreferrer" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div class="search-mask"></div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script>$(function () {
  $('span.katex-display').wrap('<div class="katex-wrap"></div>')
})</script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module" defer></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js" async></script><script src="https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js"></script><script>document.addEventListener('DOMContentLoaded', function() {
  pangu.autoSpacingPage()
})</script><script src="/js/search/local-search.js"></script><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script></body></html>